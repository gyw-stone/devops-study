

## 面试题

### 1、虾皮做了什么工作

  1）负责业务平台应用系统、业务系统监控及维护，确保线上产品正常运营

  2）日常故障，NOC投诉上报及处理，解决产品平台的突发性故障

  3）操作系统管理，扩缩容；对应用程序进行管理，包括安装、配置、部署、优化、升级等

  4）系统变更（负责审批变更【变更时间、变更内容、有无宕机时间等】）

  5）评估内部团队工具化需求 ，运维文档编写上传到知识库

### 2、python实现了什么功能，怎么去实现的

  1）自动发送到岗以及总结：首先获取google在线文档的json，并分享给创建好的项目下面的第三方；主要用的模块是gspread，re，datetime；gpsread读取google表格，获取当天日期下的参数，和获取到的邮箱合并称一个字典，用参数来读取字典中的邮箱；之后写函数保存template（发送到岗与总结的模版），根据脚本运行时间通过对应的webhook发送到对应的群里

  2）实现增量编译：

 3）对比文件一致性：新增开发的文件，清单判断

### 3、shell实现什么功能，怎么去实现的（用shell写脚本吗）

  1）服务启停：获取启动路径和文件，用read -p来收集参数，先判断是否有参数写入，再匹配case里面的参数值进行对应的操作

  2）快速认证ssh：创建循环来获取每个ip的ssh—key

  3）批量创建用户

  4）定期备份文件

\#！/bin/bash

images=(`kubeadm config images list --kubernetes-version=$version|awk -F '/' '{print $0}'`)

for imagename in ${images[@]} ; do

​        docker pull $imagename

done

  5）磁盘检测。定义一个变量=df -Ph ｜ awk  'NR==5{print int($5)}', 然后在for 循环，if 判断大于90 

 6）k8s迁移脚本

### 4、推动流程优化案例举例

  1）推动配置流程规范化：新建项目有很多配置文件不规范的情况，项目负责人进行沟通并完善流程后进行宣讲，并实施（配置分离）

  2）推动ant编译转maven编译，弄好一个ant转maven模版让开发团队根据自己项目进行更改（maven有约定的目录结构，拥有一个生命周期，拥有依赖管理，仓库管理）

  3）推进分支策略改造，项目团队的分支五花八门，不适合管理

 4）引入灰度监控模式

### 5、问题处理思路

  1）合并代码出现重复问题（两个人操作同一个文件），解决方法：对比版本，选择需要的版本，再push上去（另一份版本备份）

  2）比如监控出现问题，首先看告警信息，假如是激增，先去判断是否是压测，再去检查日志（根据架构一步一步来），看哪里报错，具体问题具体分析

  3） 编译问题：确认操作系统，版本等一致；再删除本地依赖缓存；通过（mvn dependency命令对比编译树，检查本地是否缓存错误jar包）；不通过（看编译日志，具体问题具体分析）

 4）部署问题：看日志分析，具体问题具体处理。

### 6.如何使不同主机上的容器互相通信

  1）直接路由：修改docker默认虚拟网段，分别把docker网段添加到对方的路由表上

  2）使用网卡虚拟技术macvlan

  3）走隧道（overlay, overload)（主要用的）

  4）第三方方案；常用的包括flannel、weave和calico

### 7.常用中间件以及原理等

####  nginx

nginx是一个web服务器和反向代理服务器，支持多种协议（http，https，smtp）。做反向代理可以隐藏源服务器的存在和特征，安全性高。Nginx支持gunzip模块将请求压缩到上游。多进程单线程。是7层代理（监听HTTP与HTTPS，基于内容（域名/URL）转发），不同于四层代理监听TCP与UDP基于端口和VIP转发。

优点：跨平台，配置简单，高并发连接，内存消耗小，稳定性高，健康检查，易于扩展。

缺点：处理动态能力差

```
# 反向代理配置
location /test/
{
proxy_pass http://127.0.0.1:8080; # 8080后有"/"代表不附加location路径
}
```

**epoll 原理**

连接有 I/O 流事件产生的时候，epoll 就会去告诉进程哪个连接有 I/O 流事件产生，然后进程就去处理这个链接。

**1）与apache的区别**

占用更少内存和资源，抗并发，处理请求是异步非阻塞，编写简单

**2）处理HTTP请求**

使用反应器模式，主进程发起事件等待，fork的子进程accept成功后，通过读写事件与客户端进行交互

**3）Master和Worker进程分别是什么**

Master负责读取及评估配置和维持

Worker负责处理请求

**4）为什么不适用多线程**

采用单线程来异步非阻塞处理请求，不会为每个请求分配CPU和内存资源，节省了大量资源，同时也减少了大量的CPU的上下文切换

**5）分发策略**

轮询：排队，依次轮询

IP hash：同一个hash同一个服务器处理

权重：按比例分配

最少连接：优先分配到连接数最少的服务器

第三方插件：fair、url_hash

![image-20230316200640097](C:\Users\stone\AppData\Roaming\Typora\typora-user-images\image-20230316200640097.png)

**6）nginx虚拟主机怎么区分**

基于域名的主机用域名区分

基于端口的主机用端口区分

基于IP的主机用IP区分

**7）限流做法**

正常限制访问频率

突发限制访问频率

限制并发连接数

**8）常用模块，用来做什么**

rewrite：重写

access：来源控制

ssl：安全加密

ngx_http_gzip_module: 网络传输压缩模块

ngx_http_proxy_module：代理

ngx_http_upstream_module：定义后端服务器列表

ngx_cache_purge: 缓存清除

**9）nginx -s reload实现原理**

(1) 向 master 进程发送 HUP 信号(reload命令)
(2) master 进程校验配置文件语法是否正确
(3) master 进程打开新的监听端口(如果没有定义新端口，就会继承父进程的端口）
(4) master 进程用新配置启动新的 worker 子进程
(5) master 进程向老 worker 子进程发送 QUIT 信号
(6) 老 worker 进程关闭监听句柄，处理完当前连接后结束进程

**10）nginx的使用场景**

（1）负载均衡（配置了反向代理的基础上)

（2）静态服务器

（3）HTTP服务器

（4）正向代理

（5）缓存服务：cdn

（6）虚拟主机

（7）限流

（8）高可用（nginx+keepalived)

**11) keepalive原理**

1. TCP连接建立后，每个端点都会启动一个计时器，以便在一定时间内检测连接是否处于空闲状态。
2. 如果该计时器过期，将发送一个小的keepalive数据包到对端，以请求对端确认连接处于活跃状态
3. 接收方在接收到该数据包后，会回复一个确认消息作为响应，从而让发送方知道连接是活跃地
4. 如果发送方在一定时间内没有收到任何确认信息，则假定连接失效，因此可以关闭连接

**12）做过哪些优化**

1. 调整worker_processes参数：该参数标识nginx使用的工作进程数，一般为CPU核心数的2倍

```
# 查看cpu核数
top 1
cat /proc/cpuinfo | grep ^processor /proc/cpuinfo | wc -l
```

1. 调整worker_connection参数：该参数表示每个工作进程所能处理的最大链接数，默认值为1024，可以根据需要适当调大
2. 开启TCP_NODELAY选项：该选项可以禁用Nagle算法，从而减少TCP延迟，提高网络吞吐量
3. 开启TCP_FASTOPEN选项：该选项可以在握手阶段发送部分数据，从而减少握手次数，提高首次连接速度
4. 合理使用缓存：使用nginx的proxy_cache模块或fastcgi_cache模块对静态文件或动态内容进行缓存，从而减轻后端服务器的负载
5. 压缩传输数据：使用gunzip模块对传输的数据进行压缩，从而减少传输大小，加快传输速度
6. 控制日志级别：控制access_log和error_log参数来控制日志输出级别，避免产生过多的日志信息
7. 使用HTTP/2协议：使用nginx的http2模块来支持HTTP/2协议，从而提高网络传输速度和效率
8. 配置优化：通过修改nginx的配置文件来调整一些参数，如keepalive_timeout、client_body_buffer_size等优化性能
9. 文件句柄数修改

​	1）系统全局性修改和用户局部性修改

```
 less /etc/security/limits.conf 
 #@student		-		max
 root soft nofile 65535
 root hard nofile 65535
 *	  soft nofile 25535
 *	  hard nofile 25535
# End of file
```

soft：软控制，到达设定值后，操作系统不会采取措施，只是发提醒

hard：硬控制，到达设定值后，操作系统会采取机制对当前进程进行限制，这个时候请求就会受到影响

root：代表root用户（用户局部性修改）

*：代表全局，所有用户都受此限制（全局性修改）

nofile：限制文件数的配置项

​	2）进程局部性修改

```
vim /etc/nginx/nginx.conf
worker_rlimit_nofile 35535; # 进程限制，与ulimit-n保持一致最佳
```

**13) location下的root和alias的区别**

root的处理结果是：root路径+location路径

alias的处理结果是：使用alias路径替换location路径

**14）location的优先级**

location匹配顺序为：''='' >'' ^~'' > ''~'' > ''~*'' > /

#### MongoDB

是一种提供高性能、高可用性和易于扩展的文档数据库

优点：面向文件、高性能、高可用、易扩展、可分片、对数据存储好

####  kafka

消息队列，作用是解耦，异步，削峰，pull模式。复杂度O(1)，使用TCP协议

四个核心API：

Producer API：允许一个应用程序发布一串流水地数据到若干个topic

Consumer API：允许一个应用程序订阅若干topic，并进行流处理

Streams API：允许一个应用程序作为一个流处理器，转换输入输出流

Connector API：允许构建并允许可重用地生产者或消费者，将topics连接到已存在地应用程序或数据系统

**工作原理**

Producer：生产者，消息的入口

Broker: 经纪人，kafka的一个实例，每个服务器上有若干个实例

Topic：消息主题，kafka的数据保存在topic中。每个broker上可以创建多个topic

Partition：Topic的分区（若干个），作用是负载，提高吞吐量。同一个topic在不同的分区数据是不可重复的，分区表现形式就是一个一个文件夹

Replication：分区副本。Leader（主分区）将数据同步到Follower（从）。主故障会选择一个从上位。默认副本最多10个，且副本数量不能大于Broker的数量 【不同于MQ】

Message：每一条发送的消息主体

Consumer：消费者，消息的出口

Consumer Group：消费者组。同一个分区的数据只能被消费者组中的某一个消费者消费；同一个消费者组的消费者可以消费同一个topic的不同分区的数据，提高吞吐量

Zookeeper：保存集群元信息，保证系统的可用性

**使用场景**

- 构造实时流数据管道：实时日志聚合。在系统或应用之间可靠地获取数据（相当于message queue）
- 流式数据处理：实时监控系统、在线机器学习等。将实时生成的数据写入到Kafka中，并利用Kafka的广泛生态系统来进行数据处理、计算和展示
- 存储系统

**优点**

- 高性能：高吞吐量和低延迟的数据处理，支持多种协议和通信方式
- 可靠性高：允许经过良好配置的数据复制，可以避免数据丢失和重复传输
- 可扩展性强：支持分布式部署，水平扩展，以满足大规模数据处理需求
- 生态系统好：多种增强功能的插件和工具，如Apache Flume、Apache Cassandra等
- 缓冲：有助于控制和优化数据流经过系统的速度

**缺点**

维护成本高：分布式系统，需要关注多方面如消费者负载均衡、主题管理、集群状态监控等

可靠性依赖配置：如配置错误，可能会导致消息队列错误

**1)  ack机制**

ack机制含义：producer收到多少broker的答复才算真的发送成功

0：表示producer无需等待leader的确认（吞吐量高、数据可靠性最差）

1：代表需要leader确认写入它的本地log并立即确认

-1/all：代表所有的 ISR(副本同步队列) 都完成后确认（吞吐低，数据可靠最高）

**2）如果判断节点存活**

Zookeeper通过心跳机制检查每个节点的连接

**3）什么时候会rebalance**

rebalance过程，消费者组下的所有实例都会停止工作，等待Rebalance过程完成

1. 新的consumer加入
2. 旧的consumer挂了
3. coordinator挂了，集群选举出新的coordinator
4. topic的 partition新加
5. consumer 调用unsubscrible（），取消topic的订阅

**4）ZooKeeper**

做分布式存储，Kafka使用ZooKeeper存放集群元数据、成员管理、Controller选举，以及其他一些管理类任务

如何从zookeeper迁移offset到kafka

1. 在consumer配置中设置`offsets.storage=kafka`和`dual.commit.enabled=true`
2. consumer做滚动消费，验证consumer是健康正常的
3. 在你的consumer配置中设置`dual.commit.enabled=false`。
4. consumer做滚动消费，验证consumer健康状态

回滚（从kafka到zookeeper）通过设置`offsets.storage=zookeeper`

**5）节点全挂如果保证数据不会丢失**

- 等待一个ISR的副本重新恢复正常，并选择这个副本作为领leader
- 选择第一个重新恢复正常服务的副本作为leader

**6）GUI工具**

Kafka Assistant：**http://www.redisant.cn/ka**

Kowl

Kafdrop

LogiKM：滴滴开源，中文

####  redis

redis是一个基于内存的高性能Key-Value数据库，定期通过异步操作把数据库数据flush到硬盘上保存，支持保存多种数据结构（string、list、set、sorted set、hashes），单个value最大限制是1GB。纯内存操作，具有快速和持久化的特征。NOSQL

**优点：**

- 高性能
- 丰富的数据类型
- 原子性操作，支持事务
- 可以对存入的key-value设置过期时间
- 高可用（横向扩展）

**缺点：**

- 数据库容量受内存限制
- 主节点宕机前可能有部分数据不能及时同步到从节点上

**1）数据淘汰策略**

内存数据集大小超过一定限度，执行数据淘汰策略

- volatile-lru: 从已设置过期时间的数据集中挑选最近最少使用的数据淘汰
- volatile-ttl: 从已设置过期时间的数据集中挑选将要过期的数据淘汰
- volatile-random: 从已设置过期时间的数据集中任意选择数据淘汰
- allkeys-lru: 从数据集中挑选最近最少使用的数据淘汰
- allkeys-random: 从数据集中任意选择数据淘汰
- no-enviction: 禁止驱逐数据

**2）适用场景**

- 会话缓存：持久化保存数据
- 全页缓存：数据一致性
- 队列：作为消息队列平台
- 排行榜/计数器：用set（集合）和sorted set（有序集合）
- 发布/订阅

**3）常见性能问题和解决方案**

问题1：Master写内存快照，调度rdbSave函数时会阻塞主线程，影响性能。方法：Master不写内存快照

问题2：Master AOF持久化，AOF文件过大会影响Master重启的恢复速度。方法：master不做持久化工作

问题3：主从复制性能问题 方法：slave和master在一个局域网内

问题4：解决数据同步重要性 方法：slave开启AOF备份数据，策略设置为每秒同步

**4）同步机制**

master做一次bgsave，并同时将后续修改操作记录到内存buffer，完后将rdb文件全量同步到slave节点，slave将rdb镜像加载到内存，加载完成后通知主节点将期间修改的操作记录同步到slave上重放

**5）相比Memcached的优势**

- Memcached所有的值都是简单的字符串，redis支持丰富的数据类型
- Redis 的速度比 Memcached快很多
- Redis可以持久化数据（一部分数据存储在硬盘上）

**6）缓存雪崩**

存在同一时间大量键过期，接着来的一大波请求瞬间都落在了数据库中导致连接异常

解决方案：

加锁排队

建立备份缓存，缓存A和缓存B，A设置超时时间，B不设置，先从A缓存，A没有读B，并且更新A缓存和B缓存

**7）优化**

1. 内存优化：配置maxmemory参数限制Redis使用的最大内存，避免内存占满而导致系统崩溃；设置maxmemory-policy参数选择合适的缓存淘汰策略
2. 网络优化：调整tcp-keepalive参数来防止TCP连接空闲超时关闭；通过pipeline批量操作命令，减少网络请求次数，提高吞吐量
3. 数据结构优化：根据具体业务场景选择合适的数据结构，并用合适的命令和选项来操作数据结构，以尽可能减少内存占用和CPU消耗

```
# 字符串
SET key value:设置键为key的值为value 
GET key: 获取键为key的值
INCR key: 增加键为key的值
DECT key: 减少键为key的值
# 列表
LPUSH key value: 向键为key的列表左侧添加一个元素value
RPUSH key value: 向键为key的列表右侧添加一个元素value
LPOP key: 从键为key的列表左侧弹出一个元素
RPOP key: 从键为key的列表右侧弹出一个元素
LINDEX key index: 获取键为key的列表中索引为index的元素
# 散列
HSET key field: 获取键为key的散列中field的值
```

4. 高可用性优化：Redis主从复制、哨兵机制或集群模式来实现高可用性，保证系统在单点故障或宕机情况下的正常运行
5. 日志优化：设置loglevel参数来控制日志输出级别，避免无用信息占用磁盘空间和影响空间性能（DEBUG(v)--INFO(vv)--WARN(vvv)--ERROR(vvvv)--FATAL(vvvvv)
6. 安全优化：配置密码、限制IP访问等方式来保护Redis数据的安全性

**8）常见的缓存策略**

1. Cache-Aside
2. Read-Through
3. Write-Through
4. Write-Behind

**9）Redis 工具命令**

```
#Redis-server: Redis 服务器的 daemon 启动程序
#Redis-cli: Redis 命令行操作工具
#Redis-benchmark: Redis性能测试工具，测试读写性能
Redis-benchmark -n 100000 -c 50
# 模拟50个客户端发送100000个SETs/GETs查询
#Redis-check-aof: 更新日志检查
#Redis-check-dump: 本地数据库检查
```

### 8.界面响应慢问题排查思路

#### 问题点：

1）界面渲染慢

-   网络带宽

-   浏览器性能

-   前端设计：页面请求接口过多、页面负责且内容多


2）接口超时问题

-   经常超时：耗时超过1min必须做异步方案 （用中间件如mq，将消息处理解藕）   

-   偶尔超时，同步调用接口：a：超时时间适当加长 b：增加重试机制

-   少部分响应慢：接口代码逻辑问题、数据库问题

-   大部分接口响应慢：网络或者服务器状态问题。其中服务器状态问题又包括内存、IO、CPU使用率高、端口

#### 排查思路

硬件：IO瓶颈（lotop），网络连接数过大（netstat），CPU（top），内存磁盘读写性能（lostat）

先确定是客户端还是服务端问题，如果是服务端问题：

1. chrome打开调试模式访问URL，定位加载慢的数据（网络慢、服务器慢，同域名下资源太多等）
2. 查看硬件消耗情况
3. 测试服务器的延时、丢包率等
4. 页面优化，切图，资源加载不要阻塞
5. web服务器上看日志，看哪个环节慢
6. mysql查询问题(连接数，慢查询日志，explain分析sql语句)
7. 使用cdn加缓存

**OSI七层模型排查**

1. 应用层：检查应用程序是否存在性能瓶颈，例如代码算法复杂，数据库层面
2. 表示层：检查数据格式是否合理，例如编码格式，压缩和加解密处理时间
3. 会话层：检查网络连接状态，例如端口被占用，网络延迟过高
4. 传输层：检查传输协议是否可靠，例如TCP协议中的拥塞控制、流量控制
5. 网络层：检查路由器配置是否正确，例如是否存在路由回路
6. 数据链路层：检查网络物理层是否正常，例如网线接口
7. 物理层：检查硬件设备是否正常

#### 解决思路

1）出口带宽：申请加大带宽

2）慢查询多：对接给开发或DBA优化

3）数据库响应慢：加缓存，或者优化架构读写分离

4）CDN加速

5）整体架构优化

### 9.CI/CD

  CI：持续集成，代码合并、构建、部署、测试在一起流程，不断执行这个过程，并对结果进行反馈，通过这个过程，在未上线前去反复测试，减少上线后出现bug的几率

  CD：持续交付，把最终的产品发布到生产环境上，让用户去使用，在使用的过程中反馈结果

  CD：持续部署，部署到测试环境、开发环境、预生产环境、生产环境

dev环境k8s部署：创建JIRA工单--create feature分支--gitlab--代码审查和合并代码到dev分支--触发 jenkins ci--单元测试--更新image version in Helm--gitlab--触发 helm 部署流水线--冒烟测试/集成测试

​		jenkins ci--制品库--拉取制品--k8s services调度部署《--jenkins部署到DEV Namespace

sit环境：代码审查和合并到sit分支，dev通过之后进行--gitlab--从dev分支获得helm chart--更新sit分支的helm 镜像--gitlab--触发helm 部署流水线--上传制品库--部署到sit namespace

uat环境：代码审查和合并到release分支--触发ci--单元测试--更新镜像 in helm--gitlab--触发helm部署流水线----拉取镜像版本--部署到uat namespace

生产环境：代码审查和合并master分支--获取dep分支版本--更新镜像in helm--gitlab--devops运维批准--触发helm部署流水线--拉取制品打成镜像--部署到prod namespace

**部署流程**

总体大概流程：分环境跑，从dev一层一层没有问题了到下一个环境最后跑到生产环境（银行部署流程，先部署准生产环境，测试通过上生产，测试）

具体流程：

1. 后端微服务的话给开发建repo

2. 更新helmchart

3. jenkins 构建一下

4. 去集群里检查一下有没有起

5. 更新外部访问的网关

6. 确认访问没问题了就让qa去验证

**架构链路**

jenkins+gitlab+docker+maven+harbor+sonar

### 10.配置分离

  1） maven的pom里面定义不同的profiles；打制品包时配置对应环境的命令

  2）配置变量env，通过变量去取对应环境的文件夹打包

  3）git中增加配置项目config，jenkins中增加下载该项目的任务，通过gitlab的webhook方式同步config项目文件的变更

 4）搭建配置中心系统CMDB

### 11.mysql

  **1）备份与恢复数据库：**mysqldump、xtrabackup、LVM文件系统的快照备份、tar包

 使用mysqldump的注意事项：

1. 拥有足够的权限
2. 指定正确的参数。如：--databases, --tables, --where
3. 考虑备份大小：准备足够的空间进行备份
4. 压缩备份文件：gzip等工具压缩
5. 定期备份
6. 测试还原：测试备份数据是否能还原成功

mysqldump备份时避免锁表：

1. 备份前关闭所有正在访问备份表的进程

2. 使用--single-transaction选项，将事务隔离级别设置为可重复读

3. 备份过程分成多个步骤，先备份结构表，再备份数据

4. 运行期间备份，添加--skip-opt选项

   ```
   mysqldump --skip-opt -u root --password=123456 dbname >mySQL.SQL
   ```

   解决

解决死锁问题思路

1. show engine innodb status; 查看死锁日志
2. 找出死锁SQL
3. SQL加锁分析
4. 分析死锁日志（持有什么锁，等待什么锁）

  **2）数据库维护**

​     1. 检查表键是否正确

​     2. 表删除数据后，回收所用空间，优化性能

​     3. 查看日志文件

- 错误日志：包含关闭和启动问题以及任意关键错误的细节。日志名通常为hostname.err，位于data目录中，可用—log-error更改

- 查询日志：记录所有mysql活动，日志名通常为hostname.log

- 二进制日志：记录更新过数据的所有语句

- 慢查询日志：记录缓慢的任何查询

  **3）主从复制**

​	binlog线程、io线程、sql线程

主从复制延时怎么优化：

- 优化网络
- 调整复制参数，`binlog_cache_size`,`max_binlog_size`
- 使用缓存
- 不查询主库
- 数据冗余：异步处理只查数据时，给消息队列处理
- 主从复制架构优化

**4）三范式**

- 1NF：对属性原子性约束，要求属性具有原子性，不可再分解
- 2NF：对记录的唯一性约束，要求记录有唯一标识
- 3NF：对字段冗余性的约束，任何字段不能有其他字段派生

 **5）优化思路**

1. 减少数据访问-创建并正确使用索引
2. 返回更少的数据，优化查询sql语句
3. 数据分页处理
4. 减少数据交互
5. 使用存储过程
6. 优化业务逻辑
7. 客户端处理大量复杂运算，多进程访问
8. 数据缓存，redis/memcache
9. 分库分表：方式有垂直分区、垂直分表、水平分区、水平分表

**6）常用存储引擎以及对应使用场景**

**innodb**（5.5后默认使用）

优点：灾难恢复性好，支持事务，使用行级锁，支持热备份，实现缓冲管理，并发性好

使用场景：事务支持，并要求实现并发控制，数据修改频繁（交易系统）

**mylsam**

优点：性能比Innodb好

使用场景：Web网站、数据仓储（只插入和查询），不支持事务，读数据较多或改数据较多的业务，数据一致性要求不高的业务

**memcached**

**7）主从复制至少需要3台服务器的原因**

确保数据的高可用性和可靠性，还是可以实现数据的异地备份和容灾，提高系统的安全性

1. 主服务器：负责处理所有的写入操作，并记录到Binlog
2. 从服务器1：主服务器的备份
3. 从服务器2：主服务器和从服务器1的备份

**8）mysql增量同步的思路**

使用工具：go-mysql-transfer、Canal、mysql-stream

编写脚本：对比时间，设置定时任务

1. go-mysql-transfer原理

   go-mysql-transfer将自己伪装成MySQL的Slave，向Master发送dump协议获取binlog，解析binlog并生成消息，实时发送给接收端

   ![image-20230515201946247](C:\Users\stone\AppData\Roaming\Typora\typora-user-images\image-20230515201946247.png)

   例如将表t_user同步到redis，配置如下规则

   ```sqlite
   rule:
     -
       schema: eseap #数据库名称
       table: t_user #表名称
       column_underscore_to_camel: true #列名称下划线转驼峰,默认为false
       datetime_formatter: yyyy-MM-dd HH:mm:ss #datetime、timestamp类型格式化，不填写默认yyyy-MM-dd HH:mm:ss
       value_encoder: json  #值编码类型，支持json、kv-commas、v-commas
       redis_structure: string # redis数据类型。支持string、hash、list、set类型(与redis的数据类型一致)
       redis_key_prefix: USER_ #key前缀
       redis_key_column: USER_NAME #使用哪个列的值作为key，不填写默认使用主键
   ```

2. 同步脚本。**配置读库（reader）和写库（writer）的连接地址、端口、账号、密码等**

   ```shell
   #!/bin/bash
   . /etc/profile
   # 当前时间（用于增量同步判断条件）
   curr_time=$(date -d last-day +%Y-%m-%d)
   # 读库的IP
   r_ip="255.255.255.0"
   # 读库的端口
   r_port="3306"
   # 读库的数据库名称
   r_dbname="dbname1"
   # 读库的账号
   r_username="root"
   # 读库的密码
   r_password="123456"
   # 写库的IP
   w_ip="255.255.255.0"
   # 写库的端口
   w_port="3306"
   # 写库的数据库名称
   w_dbname="dabname2"
   # 写库的账号
   w_username="root"
   # 写库的密码
   w_password="123456"
   # DataX增量同步(多个文件直接写多个执行命令)
   python /datax/bin/datax.py  /datax/job/ETL/table1.json  -p "-Dcurr_time=$curr_time -Dr_ip=$r_ip -Dr_port=$r_port -Dr_dbname=$r_dbname -Dr_username=$r_username -Dr_password=$r_password -Dw_ip=$w_ip -Dw_port=$w_port -Dw_dbname=$w_dbname -Dw_username=$w_username -Dw_password=$w_password"  >>/MySql/log/etl_sh$(date "+%Y%m%d").log  2>&1 &
   ```

**9）mysql集群的优缺点**

优点：

- 高可用：节点失效后可以快速自动转换
- 分布式体系结构，没有单点故障
- 性能：吞吐量高、延迟短
- 可扩展性强，支持在线扩容

缺点：

- 存在很多限制：不支持外键
- 备份和恢复不方便
- 成本高：软硬件成本相对较高，需要使用多个服务器和负载均衡器
- 占用磁盘空间大、内存占用率高
- 数据同步：所有节点数据同步，需要额外的开销和管理
- 数据延迟和不一致问题

### 12.//todo



### 13.jenkins pipline有哪些模块

  agent any：填写构建所需的环境

  stages：阶段纪录所有步骤，代表构建项目的阶段开头

  stage：阶段步骤，对应拉取代码、编译打包、部署发布等

  steps：步骤实现，具体实现该步骤的命令

  post：Pipeline结束时运行

   parallel: 并行执行

### 14、Docker原理

**namespace**

解决了进程、网络以及文件系统的隔离

```
# 查询路径,任何一个pid号
ls -al /proc/$pid/ns  
```

linux下的namespace有哪些：

- ipc：隔离进程间通信
- mnt：隔离文件系统挂载点
- net：隔离网络资源
- pid：隔离进程ID
- user：隔离用户和用户组的ID
- uts：控制主机名和域名的名称空间

**cgroup**

将进程按组进行管理，实现了 CPU、内存等资源的隔离

- subsystem:  内核模块，可以用来调度或者限制每个进程组的资源
- hierarchy: cgroup树，树的每个节点就是一个进程组，与多个subsystem关联。

```
# 查询路径
ls /sys/fs/cgroup
```

linux下的cgroup有哪些：

cpu：控制cpu资源分配

cpuacct：统计cgroups中的进程的cpu使用报告

cpuset：限制任务能运行在哪些CPU核上

memory：控制内存分配，并生成任务当前内存的使用报告

blkio：限制块设备的IO速率

devices：允许或拒绝cgroup中任务对设备的访问

net_cls：为cgroup中的报文设置上特定的classid标识

net_prio：网络接口设置报文的优先级

hugetlb：限制HugeTLB的使用

freezer：挂起或重启cgroup中的任务

ns：控制cgroups中的进程使用不同的namespace

**UnionFS**

联合文件系统，允许只读和可读写目录并存，就是说可同时删除和增加内容

**1) run cmd endpoint 的区别 **

RUN 在docker build 时执行

CMD在docker run 时执行，多个CMD只执行最后一个，会被覆盖

endpoint 类似CMD，但不会被覆盖（非要覆盖的话再docker run时指定参数）

**2）copy add 区别**

add 可以自动解压，支持远程url下载

copy 不支持远程下载

**3）docker支持什么网络**

bridge、host、none、container

**4）docker run 和dockerfile 里面的RUN的区别**

执行时间不同，dockerfile是在镜像创建阶段，docker run是镜像启动容器阶段

**5）容器跟虚拟化主机的区别**

VM虚拟化的是物理硬件，相比容器有访客系统

容器虚拟化的是操作系统，共享主机的内核，体积小速度快，可移植，可扩展，便携式

1. 启动速度不同。docker秒级，虚拟机几分钟
2. 性能损耗不同。docker需要资源更少
3. 系统利用率不同。docker更轻量
4. 隔离性不同。虚拟机隔离性更强，系统级隔离，docker进程间隔离
5. 安全性不同。docker安全性更弱
6. 可管理性不同。虚拟机有成熟的管理机制
7. 可用和可恢复性不同。docker高可用通过快速部署实现。虚拟化具备负载均衡、高可用、容灾、迁移和数据保护等成熟保障机制
8. 创造、删除速度不同。docker更快
9. 交付、部署速度不同。docker更快

**6）容器日志大小配置**

使用--log-opt参数限制容器日志大小

```
docker run -d \
  --name example-container \
  --log-opt max-size=10m \   # 单个日志目录最大大小为10m
  --log-opt max-file=3 \	# 日志文件数据为3个
  nginx:latest
```

或者修改配置文件daemon.json，添加以下内容

```
{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3"
  }
}
```

**7) 容器之间网络不通排查思路**

1. 确认容器网路配置是否正确：检查容器的IP地址和子网掩码是否正确，确保它们在同一个子网中
2. 检查容器之间的防火墙规则：容器之间可能存在防火墙规则限制流量，需要确认并解除规则
3. 检查主机网络配置：如果容器是通过主机网络连接的，则需要确认主机网络配置是否正确，并且主机网络没有被其他服务占用
4. 检查容器运行状态：容器是否处于健康状态
5. 检查网络连通性：telnet或nc等工具测试容器端口连通性，以确定网络是否正常
6. 检查容器日志：容器日志中可能包含有关网络错误的信息，可以根据日志内容进行排查

**8）特权模式**

```shell
# 检查容器是否有特权，有特权响应为true
docker inspect --format='{{.HostConfig.Privileged}}' [container_id]
# 使用特权模式
sudo docker run --privileged ubuntu # 特权模式运行ubuntu容器
# 测试容器是否可以访问主机
mount -t tmpfs none /mnt
df -h
```

### 15、了解或接触的开源组件

jenkins，ansible，kafka，prometheus，grafana，zabbix，redis，nginx，es

### 16、项目团队使用了哪些语言

java，javascript，html，nodejs，vue，python，c，C#

### 17、好的学习方法分享

  文件分类、官网学习、输出、沟通交流、网络视频学习

### 18、推动流程时受阻情况怎么处理

  首先自己多次推动，将心比心，以理服人；若还不行需要往上汇报，推动上级领导帮忙；规范流程化

### 19、kafka访问客户端很多次，怎么在本地主机查询

  查看系统日志/var/log，过滤查询kafka wc -l看次数是否超过阈值

### 20、查询日志某个关键字出现的次数以及指定day出现的次数

   \# less x.log | grep XX ｜ grep ‘day’ | wc -l

### 21、怎么配合做的压测

  搭建压测环境

### 22、k8s

#### 1）部署应用流程

拉取代码--》代码检查--》单元测试--》编译构建--》打镜像--》推送到harbor仓库--》拉取镜像--》生成yaml文件--》创建pod--》检查状态--》暴露应用端口--》验证

```yaml
# 生成部署的yaml文件，
kubectl create deployment java-demo --image=245684979/java-demo --dry-run=client -o yaml >deploy.yaml
```

#### 2）K8s的三种IP

- Node IP：Node节点的IP地址
- Pod IP：Pod的IP地址
- Cluster IP：Service的IP地址（VIP）

#### 3）service

##### 存在意义

服务发现（防止Pod失联）

定义Pod的访问策略（负载均衡），默认采用iptables实现负载均衡

##### 类型

ClusterIP: 默认类型，集群内部IP暴露服务，内部访问

```shell
# --port:集群内部service之间访问端口
# --target-port:pod内应用程序访问端口
# --type=NodePort:生成集群外部访问端口
kubectl expose deployment java-demo --port=80 --target-port=8080 --type=ClusterIP --dry-run=client -o yaml >svc2.yaml
```

NodePort: 暴露Node的IP和静态端口，外部访问

LoadBalancer: 公有云的负载均衡器，支持对外暴露服务

ExternlName: 通过返回外部服务的别名暴露服务，不常用

Ingress

DNS：[Kubernetes 集群 DNS 服务发现](https://developer.aliyun.com/article/779121)

#### 4）Ingress

解决大量暴露Port的NodePort痛点，做集群外部服务发现和负载均衡。将不同的URL的请求转发到后端不同的service，实现http层的业务路由机制。

与ingress-Controller的区别：将ingress比作配置文件，ingress-Controller是实际处理ingress规则的组件

#### 5）重启策略&下载策略

重启策略：

Always：总是重启

OnFailure：

Never

下载策略：

Always: 默认总是拉取最新的Image

Never

IfNotPresent

#### 6）容器之间如何相互通信，deployment 与deployment服务相互访问

容器之间通信：

1. Pod内部通信：任何容器共享相同的namespace和本地网络，直接通过localhost相互访问
2. 同节点之间的Pod通信：docker0这种网桥
3. 不同节点之间的pod通信：查找三层路由表转发。k8s网路配置采用CNI接口规范，主流的网络配置方案有：Flannel、waeve、calico、Macvlan等
4. 外部网络与Pod之间通信：service的clusterIP

deployment 之间访问：

访问A namespace下的B service的暴露端口C直接访问http://B.A:C // A

#### 7）k8s调度原理

1. 首先用户通过 Kubectl 提交创建 Pod 的 Yaml 的文件，向Kubernetes 系统发起资源请求,POST给APIServer
2. APIServer 接收到请求后把创建 Pod 的信息存储到 Etcd 中
3. Controller-Manager通过Watch接口发现pod信息更新，执行资源所依赖的拓扑结构整个，再发信息给APIServer，写入etcd，Pod状态变为可调度
4. 资源调度系统 Scheduler 定时去监控 APIServer，采用 watch 机制接收Pod可调度信息，分配最优节点（节点预选、节点优选、节点选定），之后将Pod和节点信息绑定发送给APIServer，APIServer存储到etcd中，再给kubelet
   - 节点预选：排除不符合条件的节点，如内存大小、端口等条件不满足
   - 节点优选：对预选出的节点进行优先级排序，以便选出最合适运行Pod对象的节点
   - 节点选定：从优先级排序结果中挑选出优先级最高的节点运行Pod，当这类节点多于1个时，则进行随机选择
5. kubelet收到Pod创建信息，调用CNI接口创建网络，调用CRI接口创建容器，调用CSI接口进行存储挂载

####  8）k8s健康检查方式

livenessProbe：判断容器是否存活，如果探测到为不健康，会先杀死，再按照容器的重启策略处理

- ExecAction: 在容器内执行一个命令，若返回码为0，则表明容器健康
- TCPSocketAction: 通过容器的IP地址和端口号执行TCP检查，若能建立连接，则表明容器健康
- HTTPGetAction: 通过容器的IP地址、端口号及路径调用HTTP Get方法，若响应的状态码大于等于200且小于400，则表明容器健康

ReadineeProbe：判断容器是否启动完成。如果探测失败，修改Pod的状态

#### 9）k8s yaml里面的requires和limits的作用

- requires: 对应的容器所需要的最小资源量
- limits: 对应的容器最大可用上限

#### 10）etcd

- 简述：管理配置信息和服务发现高可用分布式key-value数据库。
- 特点：
  - 简单：支持REST风格的HTTP+JSON API
  - 安全：支持HTTPS方式的访问
  - 快速：支持并发1k/s的写操作
  - 可靠：支持分布式结构，通过选举主节点来实现分布式系统一致性的算法
- 应用场景：
  - 服务发现：主要解决同一个分布式集群中的进程或服务找到对方并建立连接。了解集群中是否有进程在监听udp或tcp端口，并且通过名字就可以查找和连接
  - 消息发布与订阅
  - 负载均衡
  - 分布式通知与协调：与消息发布与订阅类似，都用到了REST接口的Watcher机制，通过注册与异步通知机制，实现分布式环境下不同系统之间的通知与协调，从而对数据变更做到实时处理
  - 分布式锁：使用Raft算法保持数据的强一致性实现分布式锁。锁服务有两种使用方式，一是保持独占， 二是控制时序
  - 集群监控与leader竞选

#### 11）kube-proxy

运行在所有节点上，监听apiserver中service和endpoint的变化情况，创建路由规则以提供服务IP和复杂均衡功能。简单理解此进程是Service的透明代理兼负载均衡器，核心功能是将到某个Service的访问请求转发到后端的多个Pod实例上

IPVS原理：使用iptables的扩展ipset，而不是直接调用iptables来生成规则链。ipset引入了带索引的数据结构，可以高效的查找和匹配

#### 12）Pod的生命周期状态

Pending：表示Pod已经同意被创建，正在等待kube-scheduler选择合适的节点创建，一般是在准备镜像

Running：pod中所有的容器已经被创建，并且至少有一个容器正在运行或者是正在启动或者是正在重启

Succeeded：表示所有容器已经成功终止，并且不会再启动

Failed：表示pod中所有容器都是非0状态退出

Unknown：无法读取pod状态，通常是Kube-controller-manager无法与pod通信

**13) gRPC协议机制**

gRPC是一种高性能、跨语言的远程过程调用(RPC)框架，可用于在不同的应用程序之间进行通信。使用HTTP/2协议来实现双向流式传输，并使用Protocol Buffers作为默认序列化协议来节省带宽和提高传输速度。

优点：

1. 高效性：基于二进制协议，节省带宽，提高传输速度
2. 多语言支持
3. 易于使用
4. 可扩展性：支持流式处理和多路复用，可以轻松地扩展到大规模分布式系统中

缺点：

1. 学习难度较高
2. 安全性不强：不支持加密，需要使用其他安全协议来保护数据传输
3. 兼容性：gRPC只能与支持HTTP/2协议的客户端和服务器进行通信

原理：

gRPC基于四个核心组件：服务定义、客户端存根、服务器存根和通道。

服务定义：定义可供客户端调用的方法及其参数和返回值类型

客户端和服务端存根：用于在客户端和服务端之间传输数据

通道：负责在客户端和服务器之间进行流式数据传输的底层网络连接

**14）pv，pvc**

PV：*PersistentVolume* ，持久化券，将共享存储定义为一种资源，属于集群级别资源，不属于任何ns，用户使用pv需要通过pvc申请。pv由管理员进行创建和配置，和具体的底层的共享存储技术的实现有关，比如Ceph、GlusterFS、NFS等

PVC：PersistentVolumeClaim，持久化券声明。属于ns中的资源，用于向PV申请存储资源。PVC消耗的是PV存储资源，可以请求特定的存储空间和访问模式

PV支持存储的类型：

- RBD：Ceph块存储
- FC：光纤存储设备
- NFS：共享存储卷
- CephFS：开源共享存储系统
- GlusterFS：开源共享存储系统
- HostPath：宿主机目录，仅能用于单机
- 公有云存储：AzureFile、AzureDisk、GCEPersistentDisk、AWSElasticBlockStore等

PV的生命周期：

- **Available：** 可用状态，尚未被 PVC 绑定。
- **Bound：** 绑定状态，已经与某个 PVC 绑定。
- **Failed：** 当删除 PVC 清理资源，自动回收卷时失败，所以处于故障状态。
- **Released：** 与之绑定的 PVC 已经被删除，但资源尚未被集群回收。

PV常用参数：

capacity：设置存储能力

```yaml
capacity:
  storage: 5Gi
```

volumeMode：存储卷模式。可选项Filesystem,Block

```yaml
volumeMode: Filesystem
```

accessModes：设置访问模式来限制应用对资源的访问权限。有ReadWriteOnce:读写，只能被单个节点挂载；ReadOnlyMany：只读，允许被多个节点挂载；ReadWriteMany：读写，多节点

```yaml
accessModes: 
  - ReadWriteOnce
```

mountOptions: 挂载参数，根据不同存储类型，设置不同的挂载参数

```yaml
mountOptions: 
  - hard
  - nfsvers=4.1
```

storageClassName: 指定存储类StorageClass资源

```
storageClassName: slow
```

persistentVolumeReclaimPolicy: 设置回收策略。Retain：保留数据，需要管理员手动清理；Recycle：删除策略；Delete：删除存储资源

**15）nfs**

概述：通过网络让不同的机器、不同的操作系统可以共享彼此的文件，通过.RPC协议实现

RPC服务：远程过程调用；在指定每个NFS功能所对应的port number，并且回报给客户端，让客户端可以连结到正确的port上

通信原理：NFS协议的通讯过程就是客户端通过RPC协议向服务器发送请求，服务器根据请求进行处理并返回相应的结果。客户端接收到结果后，在本地文件系统上进行操作。

优点：节省磁盘空间、方便部署

缺点：容易发生单点故障，高并发下性能有限，没有用户机制，明文数据

```shell
## install
sudo apt-get install nfs-kernel-server
# 设置nfs目录并修改权限
mkdir -p /nfs/mysql && sudo chown -R nobody:nogroup /nfs/mysql && sudo chmod -R 777 /nfs/mysql
# 配置nfs服务路径，/etc/exports,尾部添加<nfs directory> * (rw,sync,no_root_suqash)
sudo vim /etc/exports
# 重启服务
sudo /etc/init.d/rpcbind restart
sudo /etc/init.d/nfs-kernel-server restart
# 测试
sudo mount -t nfs 127.0.0.1:/nfs/mysql /mnt/nfs
```

**16）扩容**

纵向扩容：VPA（根据使用情况自动设置request）

横向扩容：HPA（自动伸缩组件的rpc数量）

pod的limit不够，会出现什么情况：pod启动失败，pod被强制终止，系统性能下降.

**17）service account**

Service Account为Pod中的运行的进程提供身份标识，并映射到ServiceAccount对象。向API服务器执行身份认证时，会将自己标识成某个用户

每个namespace至少一个sa，并且默认为`default`

1. 放弃API凭据的自动挂载

​	在ServiceAccount对象上设置automountServiceAccountToken:false，可以放弃在/var/run/secrets/kubernetes.io/serviceaccount/token处的自动挂载

```yaml
apiversion: v1
kind: ServiceAccount
metadata: 
  name: build-robot
automountServiceAccountToken: false
...
```

不给特定pod自动挂载API凭据。**Pod上spec的设置优先级大于SA的设置**

```yaml
apiversion: v1
kind: Pod
metadata: 
  name: my-pod
spec: 
  serviceAccountName: build-robot
  automountServiceAccountToken: false
...
```

2. 使用多个服务账号

列举当前名字空间中的ServiceAccount资源

```shell
kubectl get serviceaccounts
```

创建额外的ServiceAccount对象

```shell
kubectl apply -f - <<EOF
apiVersion: v1
kind: ServiceAccount
metadata: 
  name: build-robot
EOF
```

查询服务账号的完整信息

```shell
kubectl get serviceAccounts/build-robot -o yaml
```

清理

```she
kubectl delete serviceAccounts/build-robot
```

3. 手动创建API令牌

   ```she
   kubectl create token build-robot
   ```

   创建特殊注解`kubernetes.io/service-account.name` 的 Secret 对象

   ```she
   kubectl apply -f - <<EOF
   apiVersion: v1
   kind: Secret
   metadata:
     name: build-robot-secret
     annotations:
       kubernetes.io/service-account.name: build-robot
   type: kubernetes.io/service-account-token
   EOF
   ```

4. 添加ImagePullSecrets

​	  生成一个镜像拉取Secret

```
kubectl create secret docker-registry myregistrykey --docker-server=DUMMY_SERVER \
        --docker-username=DUMMY_USERNAME --docker-password=DUMMY_DOCKER_PASSWORD \
        --docker-email=DUMMY_DOCKER_EMAIL
```

更改名字空间的默认服务账号，将该 Secret 用作 imagePullSecret。

```shell
kubectl patch serviceaccount default -p '{"imagePullSecrets": [{"name": "myregistrykey"}]}'
```

也可以通过手动编辑该对象来实现同样的效果：

```shell
kubectl edit serviceaccount/default
```

检查

```she	
kubectl run nginx --image=nginx --restart=Never
kubectl get pod nginx -o=jsonpath='{.spec.imagePullSecrets[0].name}{"\n"}'
```



### 23、SSH工作原理

基于非对称加密方法（两个密钥加解密），服务器和客户端都会生成公钥，私钥。

1）服务器建立公钥

2）客户端请求连接

3）服务器发送公钥给客户端

4）客户端记录服务器公钥并计算自己的公私钥

5）比对一致，客户端发送自己的公钥给服务器

6）双向加解密

### 24、主机cpu100%、内存溢出，ssh不上，怎么处理

重启

云主机vnc

带外环境

其他协议（是否支持）

- 远程桌面协议（RDP）：微软开发的专门用于远程桌面连接的协议，可以让用户在本地计算机上使用远程计算机的应用程序和桌面。通过启用Windows的远程桌面功能，我们可以使用本地计算机上的Microsoft Remote Desktop或其它支持此协议的远程桌面客户端来连接虚拟机。
- Virtual Network Computing（VNC）：这是一个开源远程桌面协议，允许用户远程访问和控制其它设备，包括虚拟机。我们可以通过安装VNC服务器软件在虚拟机中开启VNC服务，然后在本地计算机上运行VNC客户端以连接虚拟机，实现图形界面的远程操作。
- 远程终端协议（RTP）：这是一种基于web的远程终端协议，允许用户使用浏览器连接到虚拟机，并在不需要在本地安装远程访问客户端的情况下进行远程访问和控制。这种协议最常用的实现是Web Shell。

### 25、监控处理流程-SOP

1. 接收到告警信息(Seatalk & 电话)，及时查看对应告警群，同时结合NOC监控面板，确定告警异常的地区和具体时间节点

2. 根据服务调用链路排查问题

   比如：直播（Client -> DNS -> SGW -> Haproxy ->Live-api(业务侧接入层)->Live-sessionbiz->Live-streamapi(后台接入层) -> Livetech-streamurl

   该告警根据live-api接入层，请求JoinSession方法返回错误码，进行统计计算告警，统计的告警码为"0|OK|10020|1022"则请求成功

3. 根据定位的地区和时间节点，登录日志平台，查询Live-sessionbiz服务内请求异常的日志，具体方法为JoinSession，可以从ERROR日志中定位具体的error_code和error_msg

4. 根据错误日志的错误信息，判断具体的异常服务

   1. 若问题在live-sessionbiz服务，优先排查容器层面。
      1. 若告警同时有容器高负载（CPU使用率 & 内存使用率），登录k8s平台，进行扩容
      2. 若扩容失败，显示配额不足，表示当前live集群无资源可提供扩容，登录K8s平台，查看当前集群live环境配额余额，配额不足联系业务侧协调其他负载较低服务缩容来支撑live-sessionbiz服务的扩容
      3. 扩容失败，报错为K8s平台侧错误下，在k8s群内反馈问题，联系k8s dod处理
   2. 若出现异常(Abnormal)的pod，则登陆k8s平台，选择live项目对应的异常服务地区，选择状态异常的pod，点击kill，等待pod重新调度
   3. 容器层没问题，根据异常信息排查，同时将异常信息提供给业务侧进行排查
      1. 若error_msg能看出明确的错误，如codis i/o timeout,tpc IP:Port failed等信息，优先排查live-sessionbiz服务下游服务live-session所依赖的中间件codis是否正常，通过codis告警，查看异常时间点内的Haproxy连接数，redis qps， instance of throughput， redis Avg(1m) Response Time by cmd 等指标是否出现异常，如有异常，则联系中间件同学协助排查
      2. 若看到query sessions error，通过MySQL告警排查集群是否异常，有异常联系DBA协助
      3. 若问题出现在业务侧服务上，则提供error_code给业务侧
   4. 若问题出现在后台LiveTech，登录日志平台，查询livetech-streamurl服务内请求异常的日志，具体方法为GetPlayURLList，可以从ERROR日志中定位出具体的error_code和error_msg，根据ERROR错误日志的错误信息，判断具体的异常服务
   5. 若问题再Livetech-streamurl服务，优先排查容器层面是否异常，再看异常信息
   6. 若LiveTech后台短时间内无法进行故障灰度，考虑执行已有的预案优先恢复业务
      1. 业务侧人工判断影响程度后，决定是否采用兜底策略
      2. 兜底策略：当链路上服务调用下级服务不可用时，人工判断并进行手动配置，确保整个服务的最低可用性，不影响开播/拉流，但是质量得不到保证

### 26、Prometheus

**基本原理**

通过HTTP协议周期抓取被监控组件的状态，任意组件只要提供对应的HTTP接口就可以接入监控

**Prometheus的架构组件**

- Prometheus Server：用于抓取指标、存储时间序列数据
- Exporters：暴露指标让任务抓取
- AlterManager：处理报警的报警组件
- PushGateway：支持Client主动push指标到PushGateway
- Prometheus Client Library：将指标数据导入 Prometheus Server 的软件库。使用客户端库可以方便地将自定义指标导入 Prometheus 监控系统。

通过exporter采集数据，写PromQL查询数据，altermanager配置告警规则(yml)，receiver可以设置为webhook、邮箱等

PromQL模板‘’[PromLabs | PromQL Cheat Sheet](https://promlabs.com/promql-cheat-sheet/)’‘

自定义exporter需要用prometheus client相关的依赖

#### 监控项

网络 （网络协议：http、dns、tcp、icmp；网络硬件：路由器、交换机等）【BlockBox Exporter;SNMP Exporter】

主机（资源用量（cpu、内存、磁盘等）【node exporter】

如：CPU整体使用率，用户态/内核态使用率；内存使用量，内存剩余量；磁盘读写次数、读写吞吐量；网卡出/入带宽，网卡出/入包量；TCP状态监控，进程端口监控

容器 （资源用量）【cAdvisor】

服务监控（nginx、PHP、tomcat、redis、memcache、mysql）【集成Prometheis Client】（延迟、错误、QPS、内部状态等）

web监控（响应时间、加载时间、渲染时间、页面是不是200）

日志监控（ELK日志）

安全监控（firewalld、WAF、安全宝、安全狗）

API监控（GET、POST、PUT、DELETE、HEAD接口的可用性、正确性、响应时间）

数据库性能方面，主要关注：

- 查询吞吐量
- 查询执行性能
- 连接情况
- 缓冲池使用情况

![image-20230515204415569](C:\Users\stone\AppData\Roaming\Typora\typora-user-images\image-20230515204415569.png)

#### 查询结果类型

Instant vector（即时数据）：包含一组时序，每个时序只有一个点

```
ep：http_requests_total
```

Range vector(区间数据)：包含一组时序，每个时序多个点

```
ep：http_requests_total[5m]
```

offset时间位移

```
ep：http_requests_total offset 5m
http_requests_total[1d] offset 1d
```

Scalar：纯量，没有时序

```
ep：count(http_requests_total)
```

####  怎么配置业务监控

1）通过写promql在yml中，可以配置好rules下所有yml都是告警规则

```
groups:
- name: hostStatsAlert
  rules:
  - alert: hostCpuUsageAlert # 告警规则名称
    expr: sum(avg without (cpu)(irate(node_cpu{mode!='idle'}[5m]))) by (instance) > 0.85 # PrmoQL语句
    for: 1m # 评估等待时间，等待期产生的告警状态为pending
    labels: # 自定义标签
      severity: page # 严重性
    annotations: # 报警信息展示
      summary: "Instance {{ $labels.instance }} CPU usgae high"
      description: "{{ $labels.instance }} CPU usage above 85% (current value: {{ $value }})"
  - alert: hostMemUsageAlert
    expr: (node_memory_MemTotal - node_memory_MemAvailable)/node_memory_MemTotal > 0.85
    for: 1m
    labels:
      severity: page
    annotations:
      summary: "Instance {{ $labels.instance }} MEM usgae high"
      description: "{{ $labels.instance }} MEM usage above 85% (current value: {{ $value }})"
```

2）写对应的promql公式，当满足某个设定的条件触发告警通过webhook、email或者电话方式通知到负责人,

```
<metric name>{<label name>=<label value>, ...}
```

```text
# 按照主机查询各个主机的CPU使用率
sum(sum(irate(node_cpu_seconds_total{mode!='idle'}[5m]))  / sum(irate(node_cpu_seconds_total[5m]))) by (instance) > 90
```

```
sum(irate(container_cpu_usage_seconds_total{}[5m])*100)by(pod) # by是只显示这一列，结果向量中只保留列出的标签，其余标签则移除
```

```text
sum(rate(mysql_global_status_commands_total{command=~"insert|update|delete"}[2m])) without (command) # without用于从计算结果中移除列举的标签，而保留其它标签
```

配置在告警平台即可

3）jobs和instance的区别

jobs：采集指标的任务

instance：jobs中的一个采集实例

4）自定义埋点采集数据

使用官方依赖开发埋点采集业务数据

```java
import io.prometheus.client.Counter;
import io.prometheus.client.Gauge;
import io.prometheus.client.Histogram;
import io.prometheus.client.Summary;
 
/**
 * 自定义指标
* @author zhucy
 */
public class Metrics {
    /**
     * 计数器可以用于记录只会增加不会减少的指标类型，比如记录应用请求的总量(http_requests_total)，
     * cpu使用时间(process_cpu_seconds_total)等。 一般而言，Counter类型的metrics指标在命名中
     * 我们使用_total结束。
     */
    public static Counter requestCounter = Counter.build()
            .name("dubbo_server_http_requests_total")
            .labelNames("application","path", "method")
            .help("Total requests.").register();
 
    /**
     * 使用Gauge可以反映应用的当前状态,例如在监控主机时，主机当前空闲的内容大小(node_memory_MemFree)，
     * 可用内存大小(node_memory_MemAvailable)。或者容器当前的CPU使用率,内存使用率。这里我们使用
     * Gauge记录当前应用正在处理的Http请求数量。
     */
    public static Gauge inprogressRequests = Gauge.build()
            .name("dubbo_server_http_inprogress_requests").labelNames("application", "path", "method")
            .help("Inprogress requests.").register();
 
    /**
     * 主要用于在指定分布范围内(Buckets)记录大小(如http request bytes)或者事件发生的次数。
     * 以请求响应时间requests_latency_seconds为例
     */
    public static Histogram requestLatencyHistogram = Histogram.build()
            .labelNames("application", "path", "method")
            .name("dubbo_server_http_requests_latency_seconds_histogram")
            .help("Request latency in seconds.")
            .register();
    public static Histogram.Timer histogramRequestTimer;
 
    /**
     * 和Histogram类似，不同在于Histogram可以通过histogram_quantile函数在服务器端计算分位数，而
     * Sumamry的分位数则是直接在客户端进行定义。因此对于分位数的计算。 Summary在通过PromQL进行查询时
     * 有更好的性能表现，而Histogram则会消耗更多的资源。相对的对于客户端而言Histogram消耗的资源更少
     */
    public static Summary requestLatency = Summary.build()
            .name("dubbo_server_http_requests_latency_seconds_summary")
            .quantile(0.5, 0.05)
            .quantile(0.9, 0.01)
            .labelNames("application", "path", "method")
            .help("Request latency in seconds.").register();
    public static Summary.Timer requestTimer;
 
    public Metrics(){
    }
}
```

#### 标准告警持续时间理解

for ： 10m表示指标超过阈值持续时间（即expr=true）10min后才会出发告警，核心目的是确保过滤掉抖动引发的瞬时异常，但依然会有潜在问题：

- 采样周期的存在，所有指标是离散的，低于阈值的指标可能未被采集到
- 采集周期和告警规则评估周期是不同的，采集到的正常值可能未被评估到

#### 时序数据库

时序数据库有：InflusDB, Prometheus(TSDB), loTDB, OpenTSDB

实现机制：

- 计数器
- B-Tree：加速查询和检索

- 数据压缩：降低存储成本

- 增量存储：降低存储成本

四种指标类型：

- Counter: 计数器，表示手机的数据是按照某个趋势变化的，通常用来监控请求数/异常数/用户登录数/订单数等
- Guage: 计量器，表示搜集的数据是瞬时的，往往用来记录内存使用率/磁盘使用率/CPU使用率/当前线程数/队列个数等等
- Histogram: 直方图，用于对一段时间范围内的数据采样，（通常是请求时间或响应大小），并能够对其指定区间以及总数进行统计
- Summary: 汇总，表示一段时间内数据采样结果

#### 高可用

部署多个Prometheus Server，使用相同的配置和exporter

Altermanager部署多个，Gossip机制进行Altermanager之间信息传递的机制，否则会出现相同告警通知被多个Altermanager重复发送

### 27、Jenkins认证git权限配置

1）ssh key认证。jenkins project credentials

2）url，用户密码

3）jenkinsfile 里面地址+用户密码

### 28、http变https怎么做

1. 获取SSL证书，从不同CA机构购买HTTPS证书（安信）

- 生成证书请求文件CSR
- 提交给CA机构认证（域名认证、企业文档认证、EV SSL认证【同时认证域名和企业文档】）

2. 获取https证书并安装

将https证书部署到服务器上

- 1）Apache文件将KEY+CER复制到文件上，修改httpd.conf文件

- 2）Tomcat先导入JKS文件，复制到服务器上，修改server.xml
- 3）IIS需要处理挂起的请求，将CER文件导入

3. 重定向301，将http重定向对应得HTTPS URL
4. 测试HTTPS网站，使用在线工具测试（SSL检查器）

### 29、配置审计

指在配置标识、配置控制、配置状态记录的基础上对所有配置项的功能及内容进行审查，以保证软件配置项的可追踪性。

目的：检查软件产品和过程是否符合标准、规格说明和规程。

主要任务：

a. 检查配置项是否完备，特别是关键的配置项是否遗漏

b. 检查所有的基线是否存在，基线产生的条件是否齐全

c. 检查每份技术文档作为某个配置项版本的描述是否精确，是否与版本一致

d. 检查每项已批准的更改是否都已实现

e. 检查每项配置项更改是否按照配置更改规程或有关标准进行

f. 检查每个配置管理人员的责任是否明确，是否尽到了应尽的责任

g. 检查配置信息安全是否受到破坏，评估安全保护机制的有效性

配置审计的类型：功能配置审计、物理配置审计

服务器配置审计（物理配置审计）

1、配置文档化。服务器配置需要文档化留存

2、配置版本化。服务器配置纳入版本管理范畴

3、配置参数化。对服务器配置进行参数化

4、配置权限控制。对文档化的服务器配置需要进行严格的权限管理

### 30、文件inode是做什么的

inode包含文件的元信息，包含：

- 文件的字节数
- 文件的拥有者User ID
- 文件的Group ID
- 文件的读、写、执行权限
- 文件的时间戳：ctime(inode上一次变动的时间)，mtime(文件内容上一次变动的时间)，atime(文件上一次打开的时间)
- 链接数：多少个文件名指向这个inode
- 文件数据block的位置

### 31、多网卡配置

centos： 进入**/etc/sysconfig/network-scripts/** 复制一份ifcfg-en* 编辑修改

#### 配置信息

TYPE="Ethernet"

BOOTPROTO="none"

DEFROUTE="yes"  #是否为默认路由，多网卡场景下一般只配置一个网卡为yes，即只有一条默认路由

IPV4_FAILURE_FATAL="no"

IPV6INIT="yes"

IPV6_AUTOCONF="yes"

IPV6_DEFROUTE="yes"

IPV6_FAILURE_FATAL="no"

IPV6_ADDR_GEN_MODE="stable-privacy"  #IPv6配置，如果没有相关配置可以删掉

NAME="ens192"   #需要调整和实际相同

UUID="48472f9d-757b-4325-ad3c-c0834758ce3a"  需要改成ens224的UUID，使用uuidgen ens224查看

DEVICE="ens192"  #需要调整和实际相同

ONBOOT="yes"    #随设备自启，视情况调整

IPADDR="172.2.216.79" 

PREFIX="24"

GATEWAY="172.2.216.254"

DNS1="114.114.114.114"

DNS2="8.8.8.8"    #按照实际网络配置进行修改

IPV6_PEERDNS="yes"

IPV6_PEERROUTES="yes"

IPV6_PRIVACY="no"   #IPv6配置，如果没有相关配置可以删掉
### 查看linux服务close_wait数量

```
netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'
```

### 32、es

**1）es的优缺点** 

优点：

- 横向可扩展性
- 分片机制提供更好的分布性
- 高可用，复制分片
- 速度快，负载能力强，搜索速度快

缺点：

- 脑裂：网络阻塞或节点处理能力饱和会导致各数据节点数据不一致（设置最小投票数量避免）
- 没有细致的权限管理机制

**2）es的使用场景**

作为存储

分布式的搜索引擎：站内搜索（电商，招聘，门户），IT系统搜索（OA，CRM，ERP）

数据分析

**3）为啥查询速度快**

倒排索引

分布式存储和搜索

多种查询方式

缓存机制

### 33、内核态、用户态

1. 用户态：CPU的Ring3级别运行，提供应用程序运行的空间，为了使应用程序访问到内核管理的资源；只能受限的访问资源（内存、文件、网络），且不允许访问外设

2. 内核态：CPU的Ring0级别运行，本质是内核，本质是内核，一种特殊的软件程序，用于控制计算机的硬件资源（进程管理 文件系统 内存管理 网络管理 驱动）；可以访问所有资源，包括外设

3. 用户态和内核态的相互切换：

​	通过系统调用，也就是使用特定的指令将控制权传递给内核态，让内核来完成相应的操作。系统调用可以通过软中断（int 0x80）或者硬件中断（例如I/O指令）来触发。当应用程序需要进行系统调用时，它会将参数传递给内核，并且将执行权限交给内核，让内核完成相关的操作。完成后，内核再将结果返回给应用程序，并将执行权限重新交还给应用程序，使其回到用户态。

​	在Linux系统中，一个进程可以通过系统调用函数（如open、read、write等）来请求内核进行某些操作，这些函数会将参数传递给内核，并通过系统调用指令（int 0x80）从用户态切换到内核态。

系统调用：操作系统提供的接口叫做系统调用
库函数：库函数实际上就是对系统调用接口的封装，提供简单的基本接口给用户
Shell：命令行，方便用户和系统交互

### 34、内核优化思路

- 资源管理：合理分配CPU、内存、I/O等资源，确保系统中没有过多的进程占用资源
- 进程调度：调整进程调度算法，优化进程的执行顺序，提高系统的响应速度
- 内存管理：对内存进行优化，减少内存碎片，提高内存使用效率
- I/O优化：优化磁盘读写操作，包括通过I/O调度器来控制I/O请求队列的排序（cfq, deadline, noop等），并对磁盘进行适当的分区、格式化和选用合适的文件系统等。
- 网络优化：调整网络协议栈，提高网络传输性能，如开启TCP/IP加速功能
- 调整内核参数：根据系统实际情况，调整内核参数，例如最大文件句柄数、最大打开文件数、最大内存大小等

**优化流程：**

1. 了解系统硬件配置、应用场景、负载特点等信息

2. 分析系统的瓶颈，找到不足之处
3. 根据分析结果，选择适当的优化方法和技术
4. 实施优化方案，并进行实验验证
5. 监测系统性能指标，评估优化效果
6. 根据实际情况，再次调整优化方案，直至达到最佳状态

### 35、保障业务高可用方法

1. 良好的系统架构涉及
2. 自动化脚本代替人工操作
3. 高效的容灾方案，两地三中心
4. 预防和监控：采取主动预防措施，及时进行监控和诊断，可以帮助我们在出现问题之前就解决他们
5. 优化告警流程。事前故障演练，做预案，事中快速响应（接口人），事后分析
6. 持续优化：持续优化业务、应用程序、硬件和网络基础设施，以确保系统能够满足不断增长的需求
7. 快速扩缩容。分布式的方法（横向，纵向）

**故障演练流程**

例行化故障演练->找出系统风险点->优化业务系统->产出可行有效的故障处理预案（容灾能力优化，故障预案明确）

故障演练：

1. 演练对象：演练的位置，可以针对应用本身，应用下游或者应用所在机器
2. 对象发生的具体故障：依赖服务故障，中间件故障，基础设施故障，机器故障，异常流量
3. 应用的预期故障应对表现：链路/场景、故障、影响、应对预案、SOP、实施预案的影响、预案解除条件、预案接触sop、预案实施失败的应对方案
4. 对应用表现的实际观察和判断：监控应用的各项指标，如异常打点，流量打点，业务曲线，机器性能等

### 36、 直播业务系统

![image-20230505215846229](C:\Users\stone\AppData\Roaming\Typora\typora-user-images\image-20230505215846229.png)

**1）与其他业务不同点**

- 实时性要求高，要求音视频数据流的实时传输，需要具有较低的延迟
- 带宽消耗大，需要能够处理大量并发连接，以保证观众可以流畅地观看直播。
- 互动性功能较多
- 可扩展性更好，以此来应对较高的并发访问和流量峰值
- 要用cdn推流，别的业务基本上直接请求公司服务器

**2）直播业务系统架构**

产品架构：主播端登录APP直播，采集音视频，推流SDK到上行加速节点，再推流到服务器进行转码和转发--CDN分发节点--播放SDK--播放设备--用户

Client-域名解析-网关层（SGW)-->应用层(Nginx+RTMP协议)-->中间件(kafka,ZK)-->缓存(redis, Memcached)-->存储(mysql)

前端用户设备--请求到SGW--k8s

直播采集端：主播通过各种设备进行音视频采集 ，并通过编码器将原始数据压缩成适合网络传输的格式

直播服务器：负责接收来自直播采集端的音视频数据流，并对其进行处理和转发（Nginx + RTMP协议）

CDN加速：加速直播流在全球范围内的传输

消息通信系统：支持主播与观众之间实时互动

数据存储和管理：包括账户信息、观众评论、统计数据等的存储和管理

**3）相关问题**

1. cdn堆积问题：提前把源文件预热到cdn中

2. 优化视频卡顿：

   1. 原因分析：

      推流帧率太低：主播手机端性能较差，或者有很占CPU的后台程序运行。正常情况FPS达到每秒15帧才能保障观看流畅，当低于10帧时判断为帧率太低（动态情况）。

      上传阻塞：上传网速问题，导致全部观众的观看体验都很卡顿

      下行不佳：观众的下载带宽跟不上或者网络波动较大

   2. 解决帧率方案：

      评判帧率太低：SDK的onStatisticsUpdate()回调中的V2TXLivePusherStatistics.fps的状态数据

      针对性优化方案：查看appcpu和systemcpu的大小；确认谁在消耗cpu，分辨率合理选择

   3. 解决上传阻塞问题

      主动提示主播，通过合适的UI交互提示

      设置合理的编码

   4. 解决下行问题

      设置合适的延迟控制方案：默认设置为流畅模式的5秒

3. 推流问题：

   1. 域名是否CNAME到了服务器地址？

      在域名管理检查CNAME，没有配置就配置

   2. 网络是否正常？

      切换网络排查端口不通问题，确认后对端口进行放行

   3. sgTime是否过期？

      设置合理的sgTime，12小时或者24小时

   4. sgSecret是否正确？

      定时生成有效的推流URL

   5. 推流URL是否被占用？

      在流管理平台查看在线流是否被推，或🈲推流被禁止

4. 播放失败问题排查

   1. 检查播放URL（推流URL和播放URL不是一个地址）
   2. 检查视频流
   3. 检查播放端
   4. 检查防火墙拦截：放行IP或端口
   5. 检查推流端

5. 拉流视频质量不清晰问题排查

   1. 原始流播放地址：排除摄像头的物理因素，查看推流的帧率和码率是否符合预期值；设置画面质量
   2. 低码率低分辨率的播放地址：先检查原始流的播放地址视频质量是否清晰。若原始流的地址视频清晰，说明客户端推流质量不错。建议调整云端的转码参数设置，按推荐码率调整转码的模版设置，提高转码流的输出码率。

**4）RTMP工作原理**

实时消息协议，底层协议是TCP

1. 摄像头捕获视频
2. 通过编码器将视频流传输到视频平台服务器
3. 视频平台处理视频流
4. 通过CDN分发到离用户最近的服务器上
5. 最后视频流就能成功的到达用户设备

**5）基本概念**

推流：主播将本地视频源和音频源推送到服务器

拉流：直播播放，指已实现直播推流之后，用指定地址将服务器中的视频源和音频源拉取播放的过程

AppName：直播的应用名称，用于区分直播流媒体文件存放路径，默认为live

转码：将视频码流转换成另一个视频码流的过程，是一种离线任务。以实现：适配更多终端、适配不同带宽、节省带宽

95峰值带宽：取计费周期内所有的5分钟带宽计点，按大小排序后去掉5%点，剩余95%的技术点取最大值为峰值带宽

### 37、数据包传输过程

1. 应用程序将数据打包成数据包
2. 数据包通过传输层协议(TCP，UDP)被传输到网络层
3. 网络层将数据包添加IP地址和端口号，并通过物理层的设备(如网卡)发送到网络上
4. 数据包经过一系列路由器和交换机的转发，最终到达目标服务器
5. 目标服务器接收到数据包后，将其解包并将数据交给应用程序处理

### 38、Linux

$#：传给脚本的参数个数

$0：脚本本身的名字

$1：传递给该shell脚本的第一个参数

$2：传递给该shell脚本的第二个参数

$@：传给脚本的所有参数的列表

$*：以一个单字符串显示所有向脚本传递的参数，与位置变量不同，参数可超过9个

$$：脚本运行的当前进程ID号

$?：显示最后命令的退出状态，0表示没有错误，其他表示有错误

: set paste # 解决复制yaml格式的缩进

**1）文件下有很多.log文件，如何删除**

find . -name *.log | xargs rm 

查找文件就用find

**2) 扩容流程**

创造分区 fdish

创造物理卷 pvcreate

扩展物理卷组 vgextend

将空闲空间分配 lvextend

**3）修改DNS的办法**

1. 网卡DNS设置

   ```shell
   DNS1="8.8.8.8"
   ```

2. DNS服务器设置，/etc/resolv.conf

   ```shell
   nameserver 8.8.8.8
   ```

3. host主机名配置, /etc/hosts

**4）DNS域名解析**

先查看浏览器缓存和本地缓存，向本地DNS服务器发起请求，根DNS服务器，顶级DNS服务器，域名服务器，返回结果

**5）linux启动流程**

1. BIOS或者UEFI自检。硬件设备检测
2. Boot Loader的加载。寻找可引导的操作系统
3. 内核的启动。初始化硬件设备、建立虚拟内存、挂载根文件系统
4. init程序的启动
5. 系统服务和用户空间的启动
6. 用户登录

**6）没有网络怎么安装**

提前下载好安装包以及对应的依赖

`yum install --downloadonly --downloaddir=/home/java java`

**7）服务器很卡排查思路**

1. 服务器网络原因

2. 服务器性能：内存和CPU

   ```shell
   # 查看内存大小
   cat /proc/meminfo | grep MemTotal
   # 查看cpu核数。总核数=物理CPU个数*每个物理CPU的核数
   # 查看cpu个数
   cat /proc/cpuinfo | grep "physical id" | sort | uniq |wc -l
   # 查看cpu中的core个数
   cat /proc/cpuinfo | grep "cpu cores" | uniq
   # 查看负载
   top
   ```

3. 数据库分析

   （1）进程列表

   ```sql
   SHOW PROCESSLIST;
   # 若进程列表中出现大量Copying to tmp table on disk状态，则查看临时文件表大小，并进行调整
   SHOW VARIABLES LIKE "%tmp_table_size%";
   SET GLOBAL tmp_table_size=33554432;
   ```

   （2）最大连接数

   ```sql
   # 查看最大连接数
   show variables like "%max_connection%";
   # 查看当前连接的使用情况
   show status like 'Threads%';
   ```

   （3）连接超时

   ```sql
   show variables like "%wait_timeout%"; # 默认8小时，通常10小时最佳
   ```

   （4）关闭DNS反向解析

   ```
   # my.cnf下[mysqld]中加入
   skip-name-resolve
   ```

   （5）表高速缓存

   ```
   # 查看打开表的数量，open_tables当前在缓存中打开的数量，opened_tables表示自启动打开的数量
   show global status like 'open%tables%';
   # 查看缓存上限值
   show variables like 'table_open_cache';
   ```

   （6）慢查询日志

   ```sql
   # 查看慢查询日志开启状态
   show variables like "%slow_query_log%";
   # 查看查询时间限制
   show variables like "long_query_time%";
   # 查看日志存储方式,FILE将日志存入文件，TABLE将日志存入数据库，日志信息默认写入mysql.slow_log表中
   show variables like "%log_output%";
   # 查询有多少慢查询记录
   show variables like "%Slow_queries%";
   ```

**8）零拷贝技术**

概述：避免CPU将数据从一块存储拷贝到另外一块存储地技术。可以减少数据拷贝和共享总线操作的次数，提高传输效率。减少用户应用程序地址空间和操作系统内核地址空间之间因为上下文切换而带来的开销。

I/O操作对比：

- 传统I/O：硬盘>内核缓冲区>用户缓冲区>内核socket缓冲区>协议引擎

- sendfile：硬盘>内核缓冲区>内核socket缓冲区>协议引擎

- sendfile（DMA收集拷贝）：硬盘>内核缓冲区>协议引擎

  DMA：系统中的一个特殊设备，可以协调完成内存到设备间的数据传输，中间过程不需要CPU介入

技术分类：

- 直接I/O
- mmap
- sendfile：在两个文件描述符之间直接传递数据（内核中操作），从而避免了数据在内核缓冲区和用户缓冲区之间的拷贝
  - 原理：sendfile()系统调用利用DMA引擎将文件数据拷贝到内核缓冲区；然后数据被拷贝到socket缓冲区；DMA将数据从内核socket缓冲区拷贝到协议引擎中
- splice

**9）上下文切换**

CPU上下文：CPU寄存器和程序计数器

- CPU寄存器是CPU内置的容量小、但速度极快的内存
- 程序计数器是用来存储CPU的指令位置（正在执行、即将执行）

上下文切换：把前一个任务的CPU上下文保存起来，加载新任务的上下文，最后跳转到程序计数器所指的新位置，允许新任务。保存的上下文，存储在内核中，并在任务重新调度执行时再次加载。保证任务原来的状态不受影响

CPU上下文切换的类型：

进程上下文切换：进程之间的切换，内核态与用户态的切换。进程调度场景有：1.某个进程时间片耗尽，会被系统挂起，切换到其他等待CPU的进程；2.进程所需系统资源不足，需要等待资源满足时才可运行，此时会被挂起，其他进程被调度；3.进程通过sleep方法主动挂起，其他进程就有机会被调度；4.有更高优先级的进程，当前进程会被挂起，高优先级进程会被调度；5.硬件中断时，CPU上的进程会被中断挂起，转而执行内核中的中断服务程序

线程上下文切换：线程之间的切换。具体场景有：1.同一进程的线程切换，只切换线程的私有数据、寄存器等不共享的数据；2.非同一进程的线程切换，与进程上下文切换类似

中断上下文切换：响应硬件的事件，中断进程的正常调度和执行，转而调用中断处理程序，不需要保存和恢复进程的虚拟内存等用户态资源，只需要处理CPU寄存器、内核堆栈等内核态的资源

分析工具：

- vmstat：`vmstat <选项> <时间间隔> <报告次数>`，主要用来分析系统的内存使用情况，CPU上下文切换和中断的次数。`vmstat 5`每隔5秒输出一次

- pidstat：查看每个进程的详细情况。`pidstat -w 5`5s显示一次进程的上下文切换情况
- /proc/interrupts文件：获取中断的详细信息`watch -d cat /proc/interrupts` watch批处理命令，默认为每2秒执行一次命令，watch -n 10 为每10s

**10）buffer和cache

buffer: 对io设备的写缓存

cache: 对io设备的读缓存

可以情况buffer和cache来做内存备用，`sync && echo 3 > /proc/sys/vm/drop_caches`

### 39、变更流程

变更申请-->变更评审-->变更审批-->变更发布-->变更验证

变更申请：变更目的、时间、持续时间、影响范围、影响地区、变更人等（通常开发提交）

变更评审：评审委员会进行评审

变更发布：infra团队进行

变更验证：让qa验证

变更预案：

1. 制定详细的变更计划，包括变更的时间、内容、影响范围、实施步骤等
2. 定义变更的风险评估和管理措施，包括针对潜在的安全漏洞、系统错误、数据丢失等问题的预防和应对方案
3. 执行测试和验证流程，确保变更不会对系统的稳定性和安全性造成负面影响
4. 联系关键利益相关者，提前告知有关变更，并明确变更的影响范围和持续时间
5. 在实施变更之前备份所有重要数据和配置文件，以便在出现问题时可以快速恢复系统
6. 监测变更后的系统性能和表现，及时发现和解决任何问题
